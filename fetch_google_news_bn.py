#!/usr/bin/env python3
"""
Google News RSS Fetcher for Bangladesh News - BANGLA ONLY (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ)
Fetches Bangla news from Google News RSS feeds focused on Bangladesh topics
No API key required - completely free!
"""

import feedparser
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict
from email.utils import parsedate_to_datetime

# Google News RSS feeds for Bangladesh - BANGLA ONLY - LATEST NEWS (past 24 hours)
GOOGLE_NEWS_FEEDS = [
    {
        "name": "Google News - ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ (‡¶∏‡¶∞‡ßç‡¶¨‡¶∂‡ßá‡¶∑)",
        "url": "https://news.google.com/rss?hl=bn&gl=BD&ceid=BD:bn",
        "language": "bn"
    },
    {
        "name": "Google News - ‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø (‡¶∏‡¶∞‡ßç‡¶¨‡¶∂‡ßá‡¶∑)",
        "url": "https://news.google.com/rss/search?q=‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂+‡¶∞‡¶æ‡¶ú‡¶®‡ßÄ‡¶§‡¶ø+when:1d&hl=bn&gl=BD&ceid=BD:bn",
        "language": "bn"
    },
    {
        "name": "Google News - ‡¶ï‡ßç‡¶∞‡¶ø‡¶ï‡ßá‡¶ü (‡¶∏‡¶∞‡ßç‡¶¨‡¶∂‡ßá‡¶∑)",
        "url": "https://news.google.com/rss/search?q=‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂+‡¶ï‡ßç‡¶∞‡¶ø‡¶ï‡ßá‡¶ü+when:1d&hl=bn&gl=BD&ceid=BD:bn",
        "language": "bn"
    },
    {
        "name": "Google News - ‡¶¢‡¶æ‡¶ï‡¶æ (‡¶∏‡¶∞‡ßç‡¶¨‡¶∂‡ßá‡¶∑)",
        "url": "https://news.google.com/rss/search?q=‡¶¢‡¶æ‡¶ï‡¶æ+when:1d&hl=bn&gl=BD&ceid=BD:bn",
        "language": "bn"
    },
    {
        "name": "Google News - ‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø (‡¶∏‡¶∞‡ßç‡¶¨‡¶∂‡ßá‡¶∑)",
        "url": "https://news.google.com/rss/search?q=‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂+‡¶Ö‡¶∞‡ßç‡¶•‡¶®‡ßÄ‡¶§‡¶ø+when:1d&hl=bn&gl=BD&ceid=BD:bn",
        "language": "bn"
    }
]

# Time filter - only show articles from last 24 hours
MAX_ARTICLE_AGE_HOURS = 24

def is_recent_article(published_date_str: str, max_hours: int = MAX_ARTICLE_AGE_HOURS) -> bool:
    """Check if article was published within the specified hours"""
    if not published_date_str:
        return True  # Include if no date (better to include than exclude)
    
    try:
        # Parse the published date
        published_date = parsedate_to_datetime(published_date_str)
        
        # Get current time
        now = datetime.now(published_date.tzinfo)
        
        # Calculate time difference
        time_diff = now - published_date
        
        # Check if within max hours
        return time_diff.total_seconds() <= (max_hours * 3600)
    except:
        return True  # Include if parsing fails

def fetch_rss_feed(feed_url: str, filter_recent: bool = True) -> List[Dict]:
    """Fetch and parse RSS feed, optionally filtering for recent articles only"""
    try:
        feed = feedparser.parse(feed_url)
        articles = []
        filtered_count = 0
        
        for entry in feed.entries:
            published = entry.get('published', '')
            
            # Filter by date if enabled
            if filter_recent and not is_recent_article(published):
                filtered_count += 1
                continue
            
            article = {
                'title': entry.get('title', 'No title'),
                'link': entry.get('link', ''),
                'published': published,
                'source': entry.get('source', {}).get('title', 'Unknown'),
                'description': entry.get('summary', '')
            }
            articles.append(article)
        
        if filtered_count > 0:
            print(f"(‡¶™‡ßÅ‡¶∞‡¶æ‡¶§‡¶® {filtered_count}‡¶ü‡¶ø ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß ‡¶´‡¶ø‡¶≤‡ßç‡¶ü‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá)", end=' ')
        
        return articles
    except Exception as e:
        print(f"‡¶´‡¶ø‡¶° ‡¶Ü‡¶®‡¶§‡ßá ‡¶§‡ßç‡¶∞‡ßÅ‡¶ü‡¶ø: {e}")
        return []

def save_articles(all_articles: Dict[str, List[Dict]], output_dir: Path):
    """Save articles to TXT and JSON files"""
    # Create timestamp-based filename
    now = datetime.now()
    time_str = now.strftime("%H-%M")
    
    # Save as TXT
    txt_file = output_dir / f"{time_str}.txt"
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write(f"Google News - ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡¶∏‡¶∞‡ßç‡¶¨‡¶∂‡ßá‡¶∑ ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ (‡¶ó‡¶§ ‡ß®‡ß™ ‡¶ò‡¶£‡ßç‡¶ü‡¶æ)\n")
        f.write(f"‡¶≠‡¶æ‡¶∑‡¶æ: ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ\n")
        f.write(f"‡¶∏‡¶Ç‡¶ó‡ßÉ‡¶π‡ßÄ‡¶§: {now.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 80 + "\n\n")
        
        total_count = sum(len(articles) for articles in all_articles.values())
        f.write(f"‡¶Æ‡ßã‡¶ü ‡¶∏‡¶Ç‡¶ó‡ßÉ‡¶π‡ßÄ‡¶§ ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß: {total_count}\n")
        f.write(f"‡¶´‡¶ø‡¶≤‡ßç‡¶ü‡¶æ‡¶∞: ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶ó‡¶§ {MAX_ARTICLE_AGE_HOURS} ‡¶ò‡¶£‡ßç‡¶ü‡¶æ‡¶∞ ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶\n\n")

        for feed_name, articles in all_articles.items():
            f.write(f"\n{'='*80}\n")
            f.write(f"üì∞ {feed_name} ({len(articles)}‡¶ü‡¶ø ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß)\n")
            f.write(f"{'='*80}\n\n")
            
            for idx, article in enumerate(articles, 1):
                f.write(f"{idx}. {article['title']}\n")
                f.write(f"   ‡¶â‡ßé‡¶∏: {article['source']}\n")
                f.write(f"   ‡¶™‡ßç‡¶∞‡¶ï‡¶æ‡¶∂‡¶ø‡¶§: {article['published']}\n")
                f.write(f"   ‡¶≤‡¶ø‡¶Ç‡¶ï: {article['link']}\n")
                if article['description']:
                    f.write(f"   ‡¶¨‡¶ø‡¶¨‡¶∞‡¶£: {article['description'][:200]}...\n")
                f.write("\n")
    
    # Save as JSON
    json_file = output_dir / f"{time_str}.json"
    json_data = {
        'timestamp': now.isoformat(),
        'language': 'bn',
        'total_articles': total_count,
        'feeds': all_articles
    }
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ {total_count}‡¶ü‡¶ø ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß ‡¶∏‡¶Ç‡¶∞‡¶ï‡ßç‡¶∑‡¶ø‡¶§ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá:")
    print(f"   üìÑ {txt_file}")
    print(f"   üìÑ {json_file}")

def main():
    """Main function to fetch Google News RSS feeds"""
    print("üåê Google News RSS ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶¨‡¶æ‡¶¶ ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...")
    print("üì∞ ‡¶≠‡¶æ‡¶∑‡¶æ: ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ\n")
    
    # Create output directory
    now = datetime.now()
    date_folder = now.strftime("%Y-%m-%d")
    output_dir = Path("output_google_news_bn") / date_folder
    output_dir.mkdir(parents=True, exist_ok=True)
    
    all_articles = {}
    
    for feed_info in GOOGLE_NEWS_FEEDS:
        feed_name = feed_info['name']
        feed_url = feed_info['url']
        
        print(f"üì° {feed_name} ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...", end=' ')
        articles = fetch_rss_feed(feed_url)
        
        if articles:
            all_articles[feed_name] = articles
            print(f"‚úÖ {len(articles)}‡¶ü‡¶ø ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß")
        else:
            print(f"‚ö†Ô∏è  ‡ß¶‡¶ü‡¶ø ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß")
    
    if all_articles:
        save_articles(all_articles, output_dir)
    else:
        print("\n‚ùå ‡¶ï‡ßã‡¶®‡ßã ‡¶´‡¶ø‡¶° ‡¶•‡ßá‡¶ï‡ßá ‡¶®‡¶ø‡¶¨‡¶®‡ßç‡¶ß ‡¶∏‡¶Ç‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡¶®‡¶ø")

if __name__ == "__main__":
    main()
